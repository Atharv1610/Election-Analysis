{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webScrape(links):\n",
    "  unique_urls = set()\n",
    "  for link in links:\n",
    "    response = requests.get(link)\n",
    "    html_content = response.content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    buttons = soup.find_all('a')\n",
    "    url = {button['href'] for button in buttons if 'href' in button.attrs}\n",
    "    unique_urls.update(url)\n",
    "\n",
    "    for link in url:\n",
    "        print(link)\n",
    "    print(f\"Total Links: {len(unique_urls)}\")\n",
    "  return unique_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://apps.apple.com/in/app/voter-helpline/id1456535004\n",
      "https://results.eci.gov.in/AcResultGenJune2024/index.htm\n",
      "https://results.eci.gov.in/PcResultGenJune2024/index.htm\n",
      "https://play.google.com/store/apps/details?id=com.eci.citizen\n",
      "https://results.eci.gov.in/AcResultGen2ndJune2024/index.htm\n",
      "index.htm\n",
      "https://results.eci.gov.in/AcResultByeJune2024/\n",
      "Total Links: 7\n"
     ]
    }
   ],
   "source": [
    "url = ['https://results.eci.gov.in/']\n",
    "urls = webScrape(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://results.eci.gov.in/AcResultGen2ndJune2024/index.htm', 'https://results.eci.gov.in/AcResultGenJune2024/index.htm', 'https://results.eci.gov.in/PcResultGenJune2024/index.htm']\n"
     ]
    }
   ],
   "source": [
    "filtered_urls = [url for url in urls if url.endswith('/index.htm')]\n",
    "print(filtered_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from https://results.eci.gov.in/AcResultGen2ndJune2024/index.htm\n",
      "Class tables found in https://results.eci.gov.in/AcResultGen2ndJune2024/index.htm:\n",
      "\n",
      "Fetching data from https://results.eci.gov.in/AcResultGenJune2024/index.htm\n",
      "Class tables found in https://results.eci.gov.in/AcResultGenJune2024/index.htm:\n",
      "\n",
      "Fetching data from https://results.eci.gov.in/PcResultGenJune2024/index.htm\n",
      "Class tables found in https://results.eci.gov.in/PcResultGenJune2024/index.htm:\n",
      "\n",
      "https://results.eci.gov.in/PcResultGenJune2024/index.htm\n"
     ]
    }
   ],
   "source": [
    "def fetch_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        page_content = response.content\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url}\")\n",
    "        return None\n",
    "\n",
    "def categorize_tables(soup, url):\n",
    "    class_table = []\n",
    "    non_class_table = []\n",
    "    \n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        if 'table' in table.get('class', []):\n",
    "            class_table.append(url) \n",
    "        else:\n",
    "            non_class_table.append(table)\n",
    "    \n",
    "    return class_table, non_class_table\n",
    "\n",
    "def extract_data(tables, category):\n",
    "    print(f\"\\nExtracting data from {category} tables...\")\n",
    "    for table in tables:\n",
    "        print(table) \n",
    "\n",
    "# Traverse each URL\n",
    "for url in filtered_urls:\n",
    "    print(f\"Fetching data from {url}\")\n",
    "    soup = fetch_data(url)\n",
    "    if soup:\n",
    "        class_tables, non_class_tables = categorize_tables(soup, url)\n",
    "        \n",
    "        # Print the URLs of class tables\n",
    "        print(f\"Class tables found in {url}:\")\n",
    "        print()\n",
    "        \n",
    "        for table_url in class_tables:\n",
    "            print(table_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://results.eci.gov.in/PcResultGenJune2024/index.htm']\n"
     ]
    }
   ],
   "source": [
    "print(class_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from https://results.eci.gov.in/PcResultGenJune2024/index.htm\n",
      "Data written to csv_files\\PcResultGenJune2024.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def fetch_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        page_content = response.content\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url}\")\n",
    "        return None\n",
    "\n",
    "def extract_data_to_csv(soup, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        tables = soup.find_all('table')\n",
    "        for table in tables:\n",
    "            if table.find('th'): \n",
    "                headers = [th.text.strip() for th in table.find_all('th')]\n",
    "                writer.writerow(headers)\n",
    "\n",
    "                rows = table.find_all('tr')[1:]  \n",
    "                for row in rows:\n",
    "                    columns = row.find_all('td')\n",
    "                    row_data = [column.text.strip() for column in columns]\n",
    "                    writer.writerow(row_data)\n",
    "\n",
    "output_folder = 'csv_files'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for url in class_tables:\n",
    "    print(f\"Fetching data from {url}\")\n",
    "    soup = fetch_data(url)\n",
    "    if soup:\n",
    "        filename = os.path.join(output_folder, f\"{url.split('/')[-2]}.csv\")  \n",
    "        extract_data_to_csv(soup, filename)\n",
    "        print(f\"Data written to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
